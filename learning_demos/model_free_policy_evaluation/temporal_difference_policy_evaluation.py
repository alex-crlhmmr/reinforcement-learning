from envs.base import TabularEnvironment
from envs.tabular.gridworld import GridWorld
from utils.utils import load_policy
from typing import Tuple, Dict
import matplotlib.pyplot as plt
import numpy as np





def td0_learning(
    env: TabularEnvironment,
    pi: np.ndarray,
    alpha: float = 0.1,
    max_iter: int = 10_000,
    gamma: float = 0.9,) -> np.ndarray:
    """
    TD(0) policy evaluation: learn an approximate value function V^π from sampled transitions.

    This function performs online TD(0) updates while following a fixed policy π:
        V(s) <- V(s) + α [ r + γ V(s') - V(s) ]
    with the bootstrap term γ V(s') omitted when the transition ends an episode.

    Episodes are generated by calling env.reset() at the beginning and after any terminal/truncated
    transition. Because the environment resets to a fixed start state (in your GridWorld),
    the learned values will be most accurate for states that are frequently visited under π
    from that start distribution.

    Parameters
    ----------
    env : TabularEnvironment
        Tabular environment implementing reset(), step(), and exposing:
          - states: iterable of valid state ids (may be sparse)
          - n_actions: number of actions
        Note: this implementation assumes state ids are integer indices usable for array indexing.
    pi : np.ndarray
        Policy to evaluate. Shape (nS, nA), where nS must be >= max(env.states)+1.
        pi[s, a] is the probability of taking action a in state s.
    alpha : float, optional
        TD step size (learning rate), by default 0.1.
    max_iter : int, optional
        Number of TD updates (transitions) to perform, by default 10_000.
    gamma : float, optional
        Discount factor in [0, 1], by default 0.9.

    Returns
    -------
    np.ndarray
        Learned value function V_pi with shape (nS,)
    """
    nS = max(env.states) + 1
    V_pi = np.zeros(nS, dtype=np.float64)

    s = env.reset()

    for _ in range(int(max_iter)):
        a = int(env.rng.choice(env.n_actions, p=pi[s]))
        next_state, reward, terminated, truncated, _ = env.step(a)
        done = terminated or truncated

        target = reward + (0.0 if done else gamma * V_pi[next_state])

        V_pi[s] += alpha * (target - V_pi[s])

        s = env.reset() if done else next_state

    return V_pi


def td0_learning_with_history(
    env: TabularEnvironment,
    pi: np.ndarray,
    V_ref: np.ndarray,
    alpha: float = 0.1,
    max_iter: int = 100_000,
    gamma: float = 0.9,
    eval_every: int = 1000,
    min_visits: int = 10,
):
    """
    TD(0) policy evaluation with convergence diagnostics over time.

    This function learns V^π using TD(0) updates from sampled transitions generated
    by following policy π, and periodically evaluates how close the current estimate
    is to a reference value function V_ref.

    Parameters
    ----------
    env : TabularEnvironment
        The TabularEnvironment environment instance.
    pi : np.ndarray
        Policy to evaluate, shape (n_states, n_actions).
        pi[s, a] = probability of taking action a in state s.
    V_ref : np.ndarray
        Reference value function to compare against (e.g., from DP),
        shape (n_states,).
    alpha : float, optional
        TD(0) step size, by default 0.1.
    max_iter : int, optional
        Number of TD updates (transitions) to perform, by default 100000.
    gamma : float, optional
        Discount factor, by default 0.9.
    eval_every : int, optional
        Evaluate diagnostics every eval_every TD updates, by default 1000.
    min_visits : int, optional
        Only include states with at least min_visits updates in the RMSE/MAE
        computation, by default 10.

    Returns
    -------
    Tuple[np.ndarray, dict]
        - V_pi : np.ndarray
            Learned value function V^π(s), shape (n_states,).
        - history : dict
            Diagnostic history containing:
              * "steps": np.ndarray of evaluation step indices
              * "rmse": np.ndarray of RMSE values
              * "mae": np.ndarray of MAE values
              * "coverage": np.ndarray of coverage values
              * "N": np.ndarray visit/update counts per state
    """
    nS = max(env.states) + 1
    V_pi = np.zeros(nS, dtype=np.float64)

    # N[s] counts how many TD updates were applied to state s
    N = np.zeros(nS, dtype=np.int64)

    steps = []
    rmse_hist = []
    mae_hist = []
    coverage_hist = []

    s = env.reset()

    for t in range(1, int(max_iter) + 1):
        a = int(env.rng.choice(env.n_actions, p=pi[s]))
        next_state, reward, terminated, truncated, _ = env.step(a)
        done = terminated or truncated

        target = reward + (0.0 if done else gamma * V_pi[next_state])
        V_pi[s] += alpha * (target - V_pi[s])
        N[s] += 1

        s = env.reset() if done else next_state

        if t % eval_every == 0 or t == 1:
            mask = np.zeros(nS, dtype=bool)
            for st in env.states:
                if env.is_terminal(st):
                    continue
                if N[st] >= min_visits:
                    mask[st] = True

            coverage = mask.sum() / max(1, len(env.states))
            if mask.sum() > 0:
                diff = V_pi[mask] - V_ref[mask]
                rmse = float(np.sqrt(np.mean(diff ** 2)))
                mae = float(np.mean(np.abs(diff)))
            else:
                rmse, mae = float("nan"), float("nan")

            steps.append(t)
            rmse_hist.append(rmse)
            mae_hist.append(mae)
            coverage_hist.append(coverage)

            if t % (5 * eval_every) == 0:
                print(f"step={t:6d} RMSE={rmse:.3f} MAE={mae:.3f} "
                      f"mask={mask.sum()} coverage={coverage*100:.1f}%")

    history = {
        "steps": np.array(steps),
        "rmse": np.array(rmse_hist),
        "mae": np.array(mae_hist),
        "coverage": np.array(coverage_hist),
        "N": N,
    }
    return V_pi, history




def td_lambda_learning(
    env: TabularEnvironment,
    pi: np.ndarray,
    alpha: float = 0.1,
    lambda_: float = 0.6,
    max_iter: int = 10_000,
    gamma: float = 0.9,) -> np.ndarray:
    """
    TD(λ) policy evaluation: learn V^π using eligibility traces from sampled transitions.

    This function performs online TD(λ) updates while following a fixed policy π.
    It maintains an eligibility trace vector e(s) and applies updates to all states
    proportionally to their current eligibility.

    With accumulating traces (as implemented here):
        e <- γ λ e
        e[s_t] <- e[s_t] + 1
        δ_t = r_{t+1} + γ V(s_{t+1}) - V(s_t)          (bootstrap term omitted if done)
        V <- V + α δ_t e

    Episodes are generated by calling env.reset() at the beginning and after any terminal/truncated
    transition. When an episode ends, eligibility traces are cleared.

    Parameters
    ----------
    env : TabularEnvironment
        Tabular environment implementing reset(), step(), and exposing:
          - states: iterable of valid state ids (may be sparse)
          - n_actions: number of actions
        State ids must be integer indices usable for array indexing.
    pi : np.ndarray
        Policy to evaluate. Shape (nS, nA), where nS must be >= max(env.states)+1.
        pi[s, a] is the probability of taking action a in state s.
    alpha : float, optional
        Step size (learning rate), by default 0.1.
    lambda_ : float, optional
        Trace-decay parameter λ in [0, 1], by default 0.6.
        - λ=0 reduces to TD(0)
        - λ close to 1 approaches Monte Carlo-like updates (higher variance, less bias)
    max_iter : int, optional
        Number of TD updates (transitions) to perform, by default 10_000.
    gamma : float, optional
        Discount factor in [0, 1], by default 0.9.

    Returns
    -------
    np.ndarray
        Learned value function V_pi with shape (nS,)
    """
    
    nS = max(env.states) + 1
    
    V_pi = np.zeros(nS, dtype=np.float64)
    e = np.zeros(nS, dtype=np.float64)
    s = env.reset()

    for _ in range(int(max_iter)):
        a = int(env.rng.choice(env.n_actions, p=pi[s]))
        next_state, reward, terminated, truncated, _ = env.step(a)
        done = terminated or truncated

        target = reward + (0.0 if done else gamma * V_pi[next_state])
        delta = target - V_pi[s]

        e *= gamma * lambda_
        e[s] += 1.0

        V_pi += alpha * delta * e

        if done:
            s = env.reset()
            e[:] = 0.0   
        else:
            s = next_state

    return V_pi


def td_lambda_learning_with_history(
    env: TabularEnvironment,
    pi: np.ndarray,
    V_ref: np.ndarray,
    alpha: float = 0.1,
    lambda_: float = 0.6,
    max_iter: int = 100000,
    gamma: float = 0.9,
    eval_every: int = 1000,
    min_visits: int = 10,
) -> Tuple[np.ndarray, Dict]:
    """
    TD(λ) policy evaluation with eligibility traces and convergence diagnostics over time.

    This function learns V^π using TD(λ) updates generated by following policy π, and
    periodically evaluates how close the current estimate is to a reference value function
    V_ref (e.g., computed by Dynamic Programming).

    Diagnostics are computed on a moving subset of states defined by:
      - states in env.states (non-wall / valid states),
      - excluding terminal states,
      - requiring at least min_visits updates where the state was the *current* state (N[s]).

    Parameters
    ----------
    env : TabularEnvironment
        Tabular environment implementing reset(), step(), and model methods used for masking:
          - states: iterable of valid state ids
          - is_terminal(s): terminal-state predicate
          - n_actions: number of actions
        State ids must be integer indices usable for array indexing.
    pi : np.ndarray
        Policy to evaluate. Shape (nS, nA), where nS must be >= max(env.states)+1.
        pi[s, a] is the probability of taking action a in state s.
    V_ref : np.ndarray
        Reference value function for comparison. Shape (nS,), where nS must be
        >= max(env.states)+1. Typically produced by DP methods (unconditioned on start state).
    alpha : float, optional
        TD(λ) step size (learning rate), by default 0.1.
    lambda_ : float, optional
        Trace-decay parameter λ in [0, 1], by default 0.6.
    max_iter : int, optional
        Number of TD updates (transitions) to perform, by default 100000.
    gamma : float, optional
        Discount factor in [0, 1], by default 0.9.
    eval_every : int, optional
        Compute diagnostics every eval_every TD updates, by default 1000.
    min_visits : int, optional
        Include state s in RMSE/MAE only if N[s] >= min_visits, by default 10.
        Here N[s] counts how many times s was the current state for a TD update.

    Returns
    -------
    Tuple[np.ndarray, Dict]
        - V_pi : np.ndarray
            Learned value function estimate, shape (nS,), where nS = max(env.states) + 1.
        - history : dict
            Diagnostic history with keys:
              * "steps": np.ndarray of evaluation step indices
              * "rmse": np.ndarray of RMSE values (on the masked subset)
              * "mae": np.ndarray of MAE values (on the masked subset)
              * "coverage": np.ndarray of coverage values
              * "N": np.ndarray of per-state update counts used for masking
    """
    nS = max(env.states) + 1
    V_pi = np.zeros(nS, dtype=np.float64)

    # eligibility trace
    e = np.zeros(nS, dtype=np.float64)

    # N[s] counts how many updates were "triggered" from state s being current
    N = np.zeros(nS, dtype=np.int64)

    steps = []
    rmse_hist = []
    mae_hist = []
    coverage_hist = []

    s = env.reset()

    for t in range(1, int(max_iter) + 1):
        a = int(env.rng.choice(env.n_actions, p=pi[s]))
        next_state, reward, terminated, truncated, _ = env.step(a)
        done = terminated or truncated

        target = reward + (0.0 if done else gamma * V_pi[next_state])
        delta = target - V_pi[s]

        # accumulating traces (your version)
        e *= gamma * lambda_
        e[s] += 1.0

        # TD(lambda) update: updates all states weighted by eligibility
        V_pi += alpha * delta * e

        # bookkeeping: count visits/updates for coverage like TD(0) code
        N[s] += 1

        if done:
            s = env.reset()
            e[:] = 0.0
        else:
            s = next_state

        # diagnostics (same structure as your TD(0) with_history)
        if t % eval_every == 0 or t == 1:
            mask = np.zeros(nS, dtype=bool)
            for st in env.states:
                if env.is_terminal(st):
                    continue
                if N[st] >= min_visits:
                    mask[st] = True

            coverage = mask.sum() / max(1, len(env.states))
            if mask.sum() > 0:
                diff = V_pi[mask] - V_ref[mask]
                rmse = float(np.sqrt(np.mean(diff ** 2)))
                mae = float(np.mean(np.abs(diff)))
            else:
                rmse, mae = float("nan"), float("nan")

            steps.append(t)
            rmse_hist.append(rmse)
            mae_hist.append(mae)
            coverage_hist.append(coverage)

            if t % (5 * eval_every) == 0:
                print(f"step={t:6d} RMSE={rmse:.3f} MAE={mae:.3f} "
                      f"mask={mask.sum()} coverage={coverage*100:.1f}%")

    history = {
        "steps": np.array(steps),
        "rmse": np.array(rmse_hist),
        "mae": np.array(mae_hist),
        "coverage": np.array(coverage_hist),
        "N": N,
    }
    return V_pi, history



def plot_td_stabilization(history: dict):
    """
    """
    steps = history["steps"]
    rmse = history["rmse"]
    mae = history["mae"]
    cov = history["coverage"]

    plt.figure(figsize=(9, 4))
    plt.plot(steps, rmse, label="RMSE vs V_ref")
    plt.plot(steps, mae, label="MAE vs V_ref")
    plt.xlabel("TD updates (transitions)")
    plt.ylabel("Error")
    plt.title("TD: Error vs Updates (stabilization)")
    plt.legend()
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(9, 3))
    plt.plot(steps, cov)
    plt.xlabel("TD updates (transitions)")
    plt.ylabel("Coverage (fraction of states with N>=min_visits)")
    plt.title("TD: How much of the state space is well-sampled")
    plt.tight_layout()
    plt.show()




def main():
    policy_path = "./outputs/gridworld/policy_iteration/optimal_policy.npz"
    pi_opt, V_opt, gamma, meta = load_policy(policy_path)

    env = GridWorld(
        height=int(meta["height"]),
        width=int(meta["width"]),
        wall_density=float(meta["wall_density"]),
        num_traps=int(meta["num_traps"]),
        slip_prob=float(meta["slip_prob"]),
        seed=None if int(meta["seed"]) == -1 else int(meta["seed"]),
    )

    nS = max(env.states) + 1
    assert pi_opt.shape[0] >= nS, f"Policy has {pi_opt.shape[0]} states, env needs {nS}"
    assert V_opt.shape[0] >= nS, f"V_opt has {V_opt.shape[0]} states, env needs {nS}"

    print(
        "[WARNING] TD(0) and TD(λ) is being run with a fixed start-state reset.\n"
        "          The learned value function V_pi will be accurate primarily\n"
        "          for states that are frequently visited under the policy.\n"
        "          It is NOT expected to match the DP value function V_ref\n"
        "          on states that are rarely or never visited.\n"
        "          This comparison is therefore conditioned on the policy's\n"
        "          visitation distribution, not global over all states.\n"
    )


    V_td0, hist_td0 = td0_learning_with_history(
        env, pi_opt, V_ref=V_opt,
        alpha=0.1,
        max_iter=2000000,     
        gamma=gamma,
        eval_every=2000,
        min_visits=10
    )

    V_td_lambda, hist_td_lambda = td_lambda_learning_with_history(
        env, pi_opt, V_ref=V_opt,
        alpha=0.1,
        lambda_=0.8,
        max_iter=2000000,     
        gamma=gamma,
        eval_every=2000,
        min_visits=10
    )



    plot_td_stabilization(hist_td0)
    plot_td_stabilization(hist_td_lambda)


if __name__ == "__main__":
    main()
